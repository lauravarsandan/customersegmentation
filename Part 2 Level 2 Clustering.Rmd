---
title: "Part 4 Level 2 Clustering"
author: "Laura Varsandan"
date: "17 December 2017"
output: 
  html_document: 
       smart: false
---

# Libraries

```{r}
library(data.table)
library(dplyr)
library(GGally)
library(kohonen)
library(cluster)
library(ggplot2)
library(ggfortify)
library(scales)
library(reshape2)
library(tibble)
library(pvclust)
library(networkD3)
library(shiny)
library(LDAvis)
library(tm)
library(lda)
library(tidytext)
```

At this stage of the analysis, we will create another level of segmtation within each of the 5 clusters derived from the departments shopping patterns. 

We will compare two approaches: One based on the aisles shopping patterns and one based on text resemblance of the product names between customers, treating customers as documents. 

# Loading the data

```{r}
#the clustering segmentation saved from the Level 1 Segmentation
user_dep_clustering <- fread('C:/Users/laura.varsandan/Visual Analytics Coursework/user_dep_clustering.csv')
user_aisles <- fread('C:/Users/laura.varsandan/Visual Analytics Coursework/user_aisles.csv')
user_words_collapsed <- fread('C:/Users/laura.varsandan/Visual Analytics Coursework/user_words_collapsed.csv')

```

First let's see which how big are our Level 1 clusters:
```{r}
table(user_dep_clustering$clusters_5)
prop.table(table(user_dep_clustering$clusters_5))
```


It looks like Cluster 2 and 3 would benefit most from another level of segmentation since they are the biggest. We also need to further segment cluster 5 as it contains customers which didn't fit into the other clusters. 

# Cluster 2 Level 2 Segmentation

```{r}
cluster_2 <- as.data.frame(user_dep_clustering[user_dep_clustering$clusters_5==2,c("user_id")])
nrow(cluster_2)
```

## Aisles Traditional Segmentation

```{r}
cluster_2_aisles <- merge(user_aisles,cluster_2,by="user_id",all.y=TRUE)
rownames(cluster_2_aisles) <- cluster_2_aisles$user_id
```


### Dimensionality reduction

Since there are 136 aisles, this is a number too high for visualisation and clustering, so the first step would be to reduce the number of aisles to a smaller,meaningful number.

Let's explore what columns should be left in.

What are the mean and variance for each Column?

```{r}
aisle_means <- as.data.frame(apply(cluster_2_aisles[,3:136],MARGIN=2,FUN=mean))
aisle_variance <- as.data.frame(apply(cluster_2_aisles[,3:136],MARGIN=2,FUN=var))
aisle_names <- as.data.frame(names(cluster_2_aisles[,3:136]))

aisle_statistics <-cbind(aisle_names,aisle_means,aisle_variance)
names(aisle_statistics) <- c("aisle_name","mean","variance")
```

Let's Visualize them

```{r}
p <- ggplot(aisle_statistics, aes(x=mean, y=variance,label=aisle_name)) + geom_point()
p <- p + scale_y_continuous(breaks=seq(0,0.0125,0.0005))
p <- p + scale_x_continuous(breaks=seq(0,0.20,0.005))
p_text <- p + geom_text()
p
p_text
#ggplot(aisle_statistics, aes(x=mean, y=variance,label=aisle_name)) + geom_point() + geom_text()
```

Since we are trying to preserve interpretability for clusters, we will not use a PCA, but instead we will pick the aisles that have a distiguishable mean and variance.In this case, let's take the ones that have a mean > 0.010 I.e. more than 1% of the content of baskets represent products from that particular aisle OR a variance > 0.0005, as that could increase the chances of it being a distinguishable aisle between customers. 

```{r}
meanigful_aisles <- aisle_statistics[aisle_statistics$mean>0.010 | aisle_statistics$variance > 0.0005,]
p <- ggplot(meanigful_aisles, aes(x=mean, y=variance,label=aisle_name)) + geom_point()
#p <- p + scale_y_continuous(breaks=seq(0,0.0125,0.0005))
#p <- p + scale_x_continuous(breaks=seq(0,0.20,0.005))
p_text <- p + geom_text()
p
p_text
```

Let's take the unmeaningful aisles out of the dataset

```{r}
names <- c("user_id",as.character(meanigful_aisles$aisle_name))
cluster_2_aisles <- select(cluster_2_aisles,names)
```


Let's see how many natural groups we could have

```{r}
for (i in 1:19){
  cl2_clara <- clara(cluster_2_aisles[,2:21],i,metric="euclidean",samples=100)
  sil <- cl2_clara$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)

```

So either 2 or 3 clusters could be the right answer. Let's see how it would look with both. 

#### 2 Clusters:

```{r}
cl2_clara <- clara(cluster_2_aisles[,2:21],2,metric="euclidean",samples=1000)

plot(cl2_clara)
```

This doesn't look like a very good clustering. Let's follow the previous approach and group customers into 100 groups first and then apply another set of groupings. 

```{r}
cl2_clara_100 <- clara(cluster_2_aisles[,2:21],100,metric="euclidean",samples=1000)

cl2_100_groups <- as.data.frame(cl2_clara_100$medoids)

cluster_2_aisles$segment_100 <- cl2_clara_100$clustering

```

Let's plot these on a PCP

```{r}
ggparcoord(data = cl2_100_groups,
                scale = "uniminmax", boxplot = TRUE, title = "Parallel Coord. Plot of Cluster 2")
```

Let's see how many natural groups we can find here:

```{r}
for (i in 1:19){
  cl2_clara_100 <- clara(cl2_100_groups,i,metric="euclidean",samples=100)
  sil <- cl2_clara_100$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)
```


It still looks like 2 groups is the right answer. 

```{r}
cl2_clara_100 <- clara(cl2_100_groups,2,metric="euclidean",samples=100)

```

```{r}
cl2_100_groups$clara_clusters <- as.factor(cl2_clara_100$clustering)

cl2_100_groups_pcp <- ggparcoord(data = cl2_100_groups, columns=1:20, groupColumn = 21, order = "anyClass")
cl2_100_groups_pcp
```

We can see that there are some ailes where the two groups clealy separate and others on which they overlapp. 

Let's see these results on a radar chart

```{r}
clara_2_groups_medoids <- as.data.frame(cl2_clara_100$medoids)
clara_2_groups_medoids$clusters <-as.character(1:2)

melt <- melt(clara_2_groups_medoids,id=c("clusters"))

melt %>%
 ggplot(aes(x=variable, y=value, group=clusters, color=clusters)) + 
 geom_polygon(fill=NA) + 
 coord_polar() + theme_bw() +
 #scale_x_discrete(labels = abbreviate) + 
 theme(axis.text.x = element_text(size = 10))
```

Let's see how a hierarchical clustering would perform. 

```{r}
d <- dist(cl2_100_groups[,1:20], method = "euclidean") # distance matrix
fit <- hclust(d, method="ward") 
plot(fit) # display dendogram
groups <- cutree(fit, k=3) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
plot(fit) + rect.hclust(fit, k=3, border="red")
```

The hierarchical clustering seems to suggest that there could be 3 clusters or even 6. This is a better outcome since the consumer group is quite large. 

Let's see how the 3 clusters look like.

```{r}
groups_3 <- cutree(fit, k=3)
cl2_100_groups$hc_clusters_3 <- groups_3

#calculating averages for each cluster
hc_clust_3_groups_avg <- aggregate(cl2_100_groups[,1:20],list(cl2_100_groups$hc_clusters_3),mean)
hc_clust_3_groups_avg$Group.1 <- NULL
hc_clust_3_groups_avg$id <- row.names(hc_clust_3_groups_avg)
hc_clust_3_groups_avg$group <- as.character("HC Clusters 3")
head(hc_clust_3_groups_avg)
```

```{r}
melt <- melt(hc_clust_3_groups_avg[,1:21],id=c("id"))

melt %>%
 ggplot(aes(x=variable, y=value, group=id, color=id)) + 
 geom_polygon(fill=NA) + 
 coord_polar() + theme_bw() +
 #scale_x_discrete(labels = abbreviate) + 
 theme(axis.text.x = element_text(size = 10))
```

Let's also see with 6 clusters

```{r}
groups_6 <- cutree(fit, k=6)
cl2_100_groups$hc_clusters_6 <- groups_6

#calculating averages for each cluster
hc_clust_6_groups_avg <- aggregate(cl2_100_groups[,1:20],list(cl2_100_groups$hc_clusters_6),mean)
hc_clust_6_groups_avg$Group.1 <- NULL
hc_clust_6_groups_avg$id <- row.names(hc_clust_6_groups_avg)
hc_clust_6_groups_avg$group <- as.character("HC Clusters 6")
head(hc_clust_6_groups_avg)
```
```{r}
melt <- melt(hc_clust_6_groups_avg[,1:21],id=c("id"))

melt %>%
 ggplot(aes(x=variable, y=value, group=id, color=id)) + 
 geom_polygon(fill=NA) + 
 coord_polar() + theme_bw() +
 #scale_x_discrete(labels = abbreviate) + 
 theme(axis.text.x = element_text(size = 10))
```

Since the 6 takes into account more patterns let's leave it at 6 and join it to the cluster_2 customers

```{r}
cl2_100_groups$group100 <- 1:100
cl_2_hc_clustering <- cl2_100_groups[,c("group100","hc_clusters_6")]

#join to raw data
cluster_2_aisles <- merge(cluster_2_aisles,cl_2_hc_clustering, by.x="segment_100",by.y="group100")
```

Let's create a spider diagram for the raw data as well

```{r}
cluster_2_aisles$hc_clusters_6 <- as.factor(cluster_2_aisles$hc_clusters_6)
#calculating averages for each cluster
hc_clust_6_groups_avg <- aggregate(cluster_2_aisles[,3:22],list(cluster_2_aisles$hc_clusters_6),mean)
hc_clust_6_groups_avg$Group.1 <- NULL
hc_clust_6_groups_avg$id <- row.names(hc_clust_6_groups_avg)
hc_clust_6_groups_avg$group <- as.character("HC Clusters 6")
head(hc_clust_6_groups_avg)
```
```{r}
melt <- melt(hc_clust_6_groups_avg[,1:21],id=c("id"))

melt %>%
 ggplot(aes(x=variable, y=value, group=id, color=id)) + 
 geom_polygon(fill=NA) + 
 coord_polar() + theme_bw() +
 #scale_x_discrete(labels = abbreviate) + 
 theme(axis.text.x = element_text(size = 10))


melt %>%
 ggplot(aes(x=variable, y=value, group=id, color=id)) + 
 geom_line() + 
 coord_polar() + theme_bw() +
 #scale_x_discrete(labels = abbreviate) + 
 theme(axis.text.x = element_text(size = 10))
```

### Words Clustering

```{r}
cluster_2_words <- merge(user_words_collapsed,cluster_2,by="user_id",all.y=TRUE)
rownames(cluster_2_words) <- cluster_2_words$user_id
cluster_2_words$V1 <- NULL
cluster_2_words$user_id <- as.numeric(cluster_2_words$user_id)
cluster_2_words <- cluster_2_words[order(cluster_2_words$user_id),]
cluster_2_words$user_id <- as.character(cluster_2_words$user_id)
head(cluster_2_words)
```

#### Pre - processing the data

Cleaning the Text
```{r}
cluster_2_words$all_words <- gsub("'", "", cluster_2_words$all_words)  # remove apostrophes
cluster_2_words$all_words <- gsub("[[:punct:]]", " ", cluster_2_words$all_words)  # replace punctuation with space
cluster_2_words$all_words <- gsub("[[:cntrl:]]", " ", cluster_2_words$all_words)  # replace control characters with space
cluster_2_words$all_words <- gsub("^[[:space:]]+", "", cluster_2_words$all_words) # remove whitespace at beginning of documents
cluster_2_words$all_words <- gsub("[[:space:]]+$", "", cluster_2_words$all_words) # remove whitespace at end of documents
cluster_2_words$all_words <- tolower(cluster_2_words$all_words)  # force to lowercase
```

Tokenizing
```{r}
doc.list <- cluster_2_words$all_words
names(doc.list) <- cluster_2_words$user_id
doc.list <- strsplit(doc.list, "[[:space:]]+")
doc.list[1]
```


```{r}
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
```


```{r}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]

```

Applying the LDA Model
```{r}
# MCMC and model tuning parameters:
K <- 4
G <- 500
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
```

Vizualising the model 

```{r}
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
```



```{r}
Cluster2_ProductTopics <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)
```

```{r}
#create the JSON object to feed the visualization:
json <- createJSON(phi = Cluster2_ProductTopics$phi, 
                   theta = Cluster2_ProductTopics$theta, 
                   doc.length = Cluster2_ProductTopics$doc.length, 
                   vocab = Cluster2_ProductTopics$vocab, 
                   term.frequency = Cluster2_ProductTopics$term.frequency)
```

```{r}
serVis(json, out.dir = 'vis', open.browser = TRUE)
```

Investigating the stage word as that seems to be very relevant for topic 3 of the 4 topic lda.

```{r}
stage <- dplyr::filter(cluster_2_words, grepl('stage', all_words))
stage[1,2]
```

Give that we lose the gluten free grup, we will remain with 4 main groups. 

#### Extracting the topics for each customer

```{r}
head(theta)
nrow(theta)
doc_topcs <- as.data.frame(theta)
doc_topcs$doc_no <- 1:nrow(theta)
names(doc_topcs) <- c("T1","T2","T3","T4","doc_no")
doc_topcs$user_id <- cluster_2_words$user_id
rownames(doc_topcs) <- doc_topcs$user_id
nrow(doc_topcs)
```

Since there are some customers that may belong to 2 topics, we will not extract the dominant topic, but we will cluster the customers according to these 4 topics. 

Since we have a large number of data points, we will use clara and then hierarchical clustering.

Let's see how many natural groups we have

```{r}
topic_matrix <- scale(doc_topcs[,1:4])

for (i in 1:19){
  cl2_clara_words <- clara(topic_matrix,i,metric="euclidean",samples=100)
  sil <- cl2_clara_words$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)
```

The highest score is for 15 clusters, but there are only 4 topics, so if we choose 15 clusters we risk loosing interpretability. Let's therefore choose 7 clusters. 


```{r}
cl2_clara_words <- clara(topic_matrix,7,metric="euclidean",samples=100)
doc_topcs$clusters <- cl2_clara_words$clustering
doc_topcs$user_id <- as.character(doc_topcs$user_id)
```

Let's see how this clustering compares to the aisles patterns. 

```{r}
cluster_2_aisles_m <- merge(cluster_2_aisles,doc_topcs, by="user_id")
head(cluster_2_aisles_m)

```

Create the sankey chart

```{r}
nodes = data.frame("name" = 
 c("Cluster 1a", # Node 0
 "Cluster 2a", # Node 1
 "Cluster 3a", # Node 2
 "Cluster 4a",
 "Cluster 5a",
 "Cluster 6a",
 "Cluster 1b", # Node 0
 "Cluster 2b", # Node 1
 "Cluster 3b", # Node 2
 "Cluster 4b",
 "Cluster 5b",
 "Cluster 6b",
 "Cluster 7b"))# Node 3

links = cluster_2_aisles_m[,c("hc_clusters_6","clusters","user_id")]

 #0, 1, 10, # Each row represents a link. The first number
 #0, 2, 20, # represents the node being conntected from. 
 #1, 3, 30, # the second number represents the node connected to.
 #2, 3, 40),# The third number is the value of the node
 
names(links) = c("source", "target", "value")
links$source <- as.numeric(as.character(links$source))
links$target <- as.numeric(links$target)
links$value <- as.numeric(links$value)

table(links$source)
table(links$target)
#Indexing the Source and Targets to 0
links$source <- links$source -1 
links$target <- links$target -1 + 6


sankeyNetwork(Links = links, Nodes = nodes,
 Source = "source", Target = "target",
 Value = "value", NodeID = "name",
 fontSize= 12, nodeWidth = 30)
```

We can see that the two types of clustering have led to the same number of clusters, but the membership of those clusters is completely different. 

Let's compare the interpretability of these two clustering methods. 

For the hierarchical clusters, we have the spider diagrams

For the LDA clusters, we will calculate the most frequent terms for each cluster.

```{r}
cluster_2_words_c <- merge(cluster_2_words,doc_topcs, by="user_id")
cluster_2_words_c <- cluster_2_words_c[,c("clusters","all_words")]
cluster_2_words_c <- cluster_2_words_c %>% group_by(clusters) %>% summarise(all_words = paste(all_words, collapse=" | "))
```

Processing the text data in order to calculate tf-idf of words for each cluster

```{r}

book_words <- cluster_2_words_c %>%
  unnest_tokens(word, all_words) %>%
  count(clusters, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(clusters) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words <- book_words %>%
  bind_tf_idf(word, clusters, n)


head(book_words)
```


HeatMap on tf

```{r}
book_words <- book_words[order(-book_words$tf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:20,]
  cl <- cl[order(-cl$tf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

HeatMap on tf-idf

```{r}
book_words <- book_words[order(-book_words$tf_idf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:15,]
  cl <- cl[order(-cl$tf_idf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf_idf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

Let's visualize the terms with the top tf-idf

```{r}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(clusters) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = clusters)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf_idf") +
  facet_wrap(~clusters, ncol = 2, scales = "free") +
  coord_flip()
```

We can see that these clusters, although they might look like they overlap much more, they also give a much better insight into the preferences for the types of products customers prefer the most and could therefore inform product choices a lot better. 

Let's proceed with the text mining clustering for the rest of the clusters as well. 
```{r}
write.csv(cluster_2_aisles_m, "cluster_2_level_2_final_clusters.csv")
```


# Cluster 3 Level 2 Segmentation

```{r}
cluster_3 <- as.data.frame(user_dep_clustering[user_dep_clustering$clusters_5==3,c("user_id")])
nrow(cluster_3)
```

## Words Clustering

```{r}
cluster_3_words <- merge(user_words_collapsed,cluster_3,by="user_id",all.y=TRUE)
rownames(cluster_3_words) <- cluster_3_words$user_id
cluster_3_words$V1 <- NULL
cluster_3_words$user_id <- as.numeric(cluster_3_words$user_id)
cluster_3_words <- cluster_3_words[order(cluster_3_words$user_id),]
cluster_3_words$user_id <- as.character(cluster_3_words$user_id)
head(cluster_3_words)
```


#### Pre - processing the data

Cleaning the Text
```{r}
cluster_3_words$all_words <- gsub("'", "", cluster_3_words$all_words)  # remove apostrophes
cluster_3_words$all_words <- gsub("[[:punct:]]", " ", cluster_3_words$all_words)  # replace punctuation with space
cluster_3_words$all_words <- gsub("[[:cntrl:]]", " ", cluster_3_words$all_words)  # replace control characters with space
cluster_3_words$all_words <- gsub("^[[:space:]]+", "", cluster_3_words$all_words) # remove whitespace at beginning of documents
cluster_3_words$all_words <- gsub("[[:space:]]+$", "", cluster_3_words$all_words) # remove whitespace at end of documents
cluster_3_words$all_words <- tolower(cluster_3_words$all_words)  # force to lowercase
```

Tokenizing
```{r}
doc.list <- cluster_3_words$all_words
names(doc.list) <- cluster_3_words$user_id
doc.list <- strsplit(doc.list, "[[:space:]]+")
```

```{r}
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
```


```{r}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]

```

Applying the LDA Model
```{r}
# MCMC and model tuning parameters:
K <- 5
G <- 500
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
```

Vizualising the model 

```{r}
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
```



```{r}
Cluster2_ProductTopics <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)
```

```{r}
#create the JSON object to feed the visualization:
json <- createJSON(phi = Cluster2_ProductTopics$phi, 
                   theta = Cluster2_ProductTopics$theta, 
                   doc.length = Cluster2_ProductTopics$doc.length, 
                   vocab = Cluster2_ProductTopics$vocab, 
                   term.frequency = Cluster2_ProductTopics$term.frequency)
```

```{r}
serVis(json, out.dir = 'vis', open.browser = TRUE)
```


#### Extracting the topics for each customer

```{r}
head(theta)
nrow(theta)
doc_topcs <- as.data.frame(theta)
doc_topcs$doc_no <- 1:nrow(theta)
names(doc_topcs) <- c("T1","T2","T3","T4","T5","doc_no")
doc_topcs$user_id <- cluster_3_words$user_id
rownames(doc_topcs) <- doc_topcs$user_id
nrow(doc_topcs)
```

Since there are some customers that may belong to 2 topics, we will not extract the dominant topic, but we will cluster the customers according to these 5 topics. 

Since we have a large number of data points, we will use clara.

Let's see how many natural groups we have

```{r}
topic_matrix <- doc_topcs[,1:5]
topic_matrix <- scale(topic_matrix)

for (i in 1:19){
  cl3_clara_words <- clara(topic_matrix,i,metric="euclidean",samples=100)
  sil <- cl3_clara_words$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)
```


Let's first see how the words look with 11 clusters then 4 then 5

```{r}
cl3_clara_words <- clara(topic_matrix,5,metric="euclidean",samples=100)
doc_topcs$clusters <- cl3_clara_words$clustering
cluster_3_words_m <- merge(cluster_3_words, doc_topcs, by="user_id")
cluster_3_words_c <- cluster_3_words_m[,c("clusters","all_words")]
cluster_3_words_c <- cluster_3_words_c %>% group_by(clusters) %>% summarise(all_words = paste(all_words, collapse=" | "))
```

Let's see what words are the most relevant for each cluster. 

```{r}

book_words <- cluster_3_words_c %>%
  unnest_tokens(word, all_words) %>%
  count(clusters, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(clusters) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words <- book_words %>%
  bind_tf_idf(word, clusters, n)


head(book_words)


```
HeatMap on tf

```{r}
book_words <- book_words[order(-book_words$tf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:20,]
  cl <- cl[order(-cl$tf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

HeatMap on tf-idf

```{r}
book_words <- book_words[order(-book_words$tf_idf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:15,]
  cl <- cl[order(-cl$tf_idf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf_idf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```





Let's save the results

```{r}
write.csv(cluster_3_words_m,"cluster_3_level_2_final_clusters.csv")
```


# Cluster 5 Level 2 Segmentation

```{r}
cluster_5 <- as.data.frame(user_dep_clustering[user_dep_clustering$clusters_5==5,c("user_id")])
nrow(cluster_5)
```

## Words Clustering

```{r}
cluster_5_words <- merge(user_words_collapsed,cluster_5,by="user_id",all.y=TRUE)
rownames(cluster_5_words) <- cluster_5_words$user_id
cluster_5_words$V1 <- NULL
cluster_5_words$user_id <- as.numeric(cluster_5_words$user_id)
cluster_5_words <- cluster_5_words[order(cluster_5_words$user_id),]
cluster_5_words$user_id <- as.character(cluster_5_words$user_id)
head(cluster_5_words)
```

#### Pre - processing the data

Cleaning the Text
```{r}
cluster_5_words$all_words <- gsub("'", "", cluster_5_words$all_words)  # remove apostrophes
cluster_5_words$all_words <- gsub("[[:punct:]]", " ", cluster_5_words$all_words)  # replace punctuation with space
cluster_5_words$all_words <- gsub("[[:cntrl:]]", " ", cluster_5_words$all_words)  # replace control characters with space
cluster_5_words$all_words <- gsub("^[[:space:]]+", "", cluster_5_words$all_words) # remove whitespace at beginning of documents
cluster_5_words$all_words <- gsub("[[:space:]]+$", "", cluster_5_words$all_words) # remove whitespace at end of documents
cluster_5_words$all_words <- tolower(cluster_5_words$all_words)  # force to lowercase
```

Tokenizing
```{r}
doc.list <- cluster_5_words$all_words
names(doc.list) <- cluster_5_words$user_id
doc.list <- strsplit(doc.list, "[[:space:]]+")
```

```{r}
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
```


```{r}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]

```

Applying the LDA Model
```{r}
# MCMC and model tuning parameters:
K <- 5
G <- 500
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
```

Vizualising the model 

```{r}
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
```



```{r}
Cluster2_ProductTopics <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)
```

```{r}
#create the JSON object to feed the visualization:
json <- createJSON(phi = Cluster2_ProductTopics$phi, 
                   theta = Cluster2_ProductTopics$theta, 
                   doc.length = Cluster2_ProductTopics$doc.length, 
                   vocab = Cluster2_ProductTopics$vocab, 
                   term.frequency = Cluster2_ProductTopics$term.frequency)
```

```{r}
serVis(json, out.dir = 'vis', open.browser = TRUE)
```
#### Extracting the topics for each customer

```{r}
head(theta)
nrow(theta)
doc_topcs <- as.data.frame(theta)
doc_topcs$doc_no <- 1:nrow(theta)
names(doc_topcs) <- c("T1","T2","T3","T4","T5","doc_no")
doc_topcs$user_id <- cluster_5_words$user_id
rownames(doc_topcs) <- doc_topcs$user_id
nrow(doc_topcs)
```

Since there are some customers that may belong to 2 topics, we will not extract the dominant topic, but we will cluster the customers according to these 5 topics. 

Since we have a large number of data points, we will use clara.

Let's see how many natural groups we have

```{r}
topic_matrix <- doc_topcs[,1:5]
topic_matrix <- scale(topic_matrix)

for (i in 1:19){
  cl5_clara_words <- clara(topic_matrix,i,metric="euclidean",samples=100)
  sil <- cl5_clara_words$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)
```

Let's try with 3 clusters first

```{r}
cl5_clara_words <- clara(topic_matrix,3,metric="euclidean",samples=100)
doc_topcs$clusters <- cl5_clara_words$clustering
cluster_5_words_m <- merge(cluster_5_words, doc_topcs, by="user_id")
cluster_5_words_c <- cluster_5_words_m[,c("clusters","all_words")]
cluster_5_words_c <- cluster_5_words_c %>% group_by(clusters) %>% summarise(all_words = paste(all_words, collapse=" | "))
```

Let's see what words are the most relevant for each cluster. 

```{r}

book_words <- cluster_5_words_c %>%
  unnest_tokens(word, all_words) %>%
  count(clusters, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(clusters) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words <- book_words %>%
  bind_tf_idf(word, clusters, n)


head(book_words)


```

HeatMap on tf

```{r}
book_words <- book_words[order(-book_words$tf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:20,]
  cl <- cl[order(-cl$tf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

HeatMap on tf-idf

```{r}
book_words <- book_words[order(-book_words$tf_idf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:15,]
  cl <- cl[order(-cl$tf_idf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf_idf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

Let's save the results


```{r}
write.csv(cluster_5_words_m,"cluster_5_level_2_final_clusters.csv")
```


## Cluster 4 Level 2 segmentation

```{r}

cluster_4 <- as.data.frame(user_dep_clustering[user_dep_clustering$clusters_5==4,c("user_id")])
nrow(cluster_4)
```

## Words Clustering

```{r}
cluster_4_words <- merge(user_words_collapsed,cluster_4,by="user_id",all.y=TRUE)
rownames(cluster_4_words) <- cluster_4_words$user_id
cluster_4_words$V1 <- NULL
cluster_4_words$user_id <- as.numeric(cluster_4_words$user_id)
cluster_4_words <- cluster_4_words[order(cluster_4_words$user_id),]
cluster_4_words$user_id <- as.character(cluster_4_words$user_id)
head(cluster_4_words)
```

#### Pre - processing the data

Cleaning the Text
```{r}
cluster_4_words$all_words <- gsub("'", "", cluster_4_words$all_words)  # remove apostrophes
cluster_4_words$all_words <- gsub("[[:punct:]]", " ", cluster_4_words$all_words)  # replace punctuation with space
cluster_4_words$all_words <- gsub("[[:cntrl:]]", " ", cluster_4_words$all_words)  # replace control characters with space
cluster_4_words$all_words <- gsub("^[[:space:]]+", "", cluster_4_words$all_words) # remove whitespace at beginning of documents
cluster_4_words$all_words <- gsub("[[:space:]]+$", "", cluster_4_words$all_words) # remove whitespace at end of documents
cluster_4_words$all_words <- tolower(cluster_4_words$all_words)  # force to lowercase
```

Tokenizing
```{r}
doc.list <- cluster_4_words$all_words
names(doc.list) <- cluster_4_words$user_id
doc.list <- strsplit(doc.list, "[[:space:]]+")
```

```{r}
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
```


```{r}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]

```

Applying the LDA Model
```{r}
# MCMC and model tuning parameters:
K <- 4
G <- 500
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
```

Vizualising the model 

```{r}
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
```



```{r}
Cluster2_ProductTopics <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)
```

```{r}
#create the JSON object to feed the visualization:
json <- createJSON(phi = Cluster2_ProductTopics$phi, 
                   theta = Cluster2_ProductTopics$theta, 
                   doc.length = Cluster2_ProductTopics$doc.length, 
                   vocab = Cluster2_ProductTopics$vocab, 
                   term.frequency = Cluster2_ProductTopics$term.frequency)
```

```{r}
serVis(json, out.dir = 'vis', open.browser = TRUE)
```

#### Extracting the topics for each customer

```{r}
head(theta)
nrow(theta)
doc_topcs <- as.data.frame(theta)
doc_topcs$doc_no <- 1:nrow(theta)
names(doc_topcs) <- c("T1","T2","T3","T4","doc_no")
doc_topcs$user_id <- cluster_4_words$user_id
rownames(doc_topcs) <- doc_topcs$user_id
nrow(doc_topcs)
```

Let's see how many natural groups we have

```{r}
topic_matrix <- doc_topcs[,1:4]
topic_matrix <- scale(topic_matrix)

for (i in 1:19){
  cl4_clara_words <- clara(topic_matrix,i,metric="euclidean",samples=100)
  sil <- cl4_clara_words$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)
```

Let's pick 3 clusters. 


```{r}
cl4_clara_words <- clara(topic_matrix,3,metric="euclidean",samples=100)
doc_topcs$clusters <- cl4_clara_words$clustering
cluster_4_words_m <- merge(cluster_4_words, doc_topcs, by="user_id")
cluster_4_words_c <- cluster_4_words_m[,c("clusters","all_words")]
cluster_4_words_c <- cluster_4_words_c %>% group_by(clusters) %>% summarise(all_words = paste(all_words, collapse=" | "))
```

Let's see what words are the most relevant for each cluster. 

```{r}

book_words <- cluster_4_words_c %>%
  unnest_tokens(word, all_words) %>%
  count(clusters, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(clusters) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words <- book_words %>%
  bind_tf_idf(word, clusters, n)


head(book_words)


```

HeatMap on tf

```{r}
book_words <- book_words[order(-book_words$tf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:20,]
  cl <- cl[order(-cl$tf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p

```

HeatMap on tf-idf


HeatMap on tf-idf

```{r}
book_words <- book_words[order(-book_words$tf_idf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:15,]
  cl <- cl[order(-cl$tf_idf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf_idf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

Let's save the results


```{r}
write.csv(cluster_4_words_m,"cluster_4_level_2_final_clusters.csv")
```

# Cluster 1 Level 2 Segmentation

```{r}

cluster_1 <- as.data.frame(user_dep_clustering[user_dep_clustering$clusters_5==1,c("user_id")])
nrow(cluster_1)
```

## Words Clustering

```{r}
cluster_1_words <- merge(user_words_collapsed,cluster_1,by="user_id",all.y=TRUE)
rownames(cluster_1_words) <- cluster_1_words$user_id
cluster_1_words$V1 <- NULL
cluster_1_words$user_id <- as.numeric(cluster_1_words$user_id)
cluster_1_words <- cluster_1_words[order(cluster_1_words$user_id),]
cluster_1_words$user_id <- as.character(cluster_1_words$user_id)
head(cluster_1_words)
```

#### Pre - processing the data

Cleaning the Text
```{r}
cluster_1_words$all_words <- gsub("'", "", cluster_1_words$all_words)  # remove apostrophes
cluster_1_words$all_words <- gsub("[[:punct:]]", " ", cluster_1_words$all_words)  # replace punctuation with space
cluster_1_words$all_words <- gsub("[[:cntrl:]]", " ", cluster_1_words$all_words)  # replace control characters with space
cluster_1_words$all_words <- gsub("^[[:space:]]+", "", cluster_1_words$all_words) # remove whitespace at beginning of documents
cluster_1_words$all_words <- gsub("[[:space:]]+$", "", cluster_1_words$all_words) # remove whitespace at end of documents
cluster_1_words$all_words <- tolower(cluster_1_words$all_words)  # force to lowercase
```

Tokenizing
```{r}
doc.list <- cluster_1_words$all_words
names(doc.list) <- cluster_1_words$user_id
doc.list <- strsplit(doc.list, "[[:space:]]+")
```

```{r}
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
```


```{r}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]

```

Applying the LDA Model
```{r}
# MCMC and model tuning parameters:
K <- 3
G <- 500
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
```

Vizualising the model 

```{r}
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
```



```{r}
Cluster2_ProductTopics <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)
```

```{r}
#create the JSON object to feed the visualization:
json <- createJSON(phi = Cluster2_ProductTopics$phi, 
                   theta = Cluster2_ProductTopics$theta, 
                   doc.length = Cluster2_ProductTopics$doc.length, 
                   vocab = Cluster2_ProductTopics$vocab, 
                   term.frequency = Cluster2_ProductTopics$term.frequency)
```

```{r}
serVis(json, out.dir = 'vis', open.browser = TRUE)
```

#### Extracting the topics for each customer

```{r}
head(theta)
nrow(theta)
doc_topcs <- as.data.frame(theta)
doc_topcs$doc_no <- 1:nrow(theta)
names(doc_topcs) <- c("T1","T2","T3","doc_no")
doc_topcs$user_id <- cluster_1_words$user_id
rownames(doc_topcs) <- doc_topcs$user_id
nrow(doc_topcs)
```

Let's see how many natural groups we have

```{r}
topic_matrix <- doc_topcs[,1:3]
topic_matrix <- scale(topic_matrix)

for (i in 1:19){
  cl4_clara_words <- clara(topic_matrix,i,metric="euclidean",samples=100)
  sil <- cl4_clara_words$silinfo$avg.width
  if (i==1){
  sil_data <- sil
  } else {sil_data <- c(sil_data,sil)}
}

sil_data <- as.data.frame(sil_data)
sil_data$clusters <-1:18

plot(sil_data$clusters,sil_data$sil_data)
```

Let's try with 2:

```{r}
cl1_clara_words <- clara(topic_matrix,2,metric="euclidean",samples=100)
doc_topcs$clusters <- cl1_clara_words$clustering
cluster_1_words_m <- merge(cluster_1_words, doc_topcs, by="user_id")
cluster_1_words_c <- cluster_1_words_m[,c("clusters","all_words")]
cluster_1_words_c <- cluster_1_words_c %>% group_by(clusters) %>% summarise(all_words = paste(all_words, collapse=" | "))
```

Let's see what words are the most relevant for each cluster. 

```{r}

book_words <- cluster_1_words_c %>%
  unnest_tokens(word, all_words) %>%
  count(clusters, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(clusters) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words <- book_words %>%
  bind_tf_idf(word, clusters, n)


head(book_words)


```

HeatMap on tf

```{r}
book_words <- book_words[order(-book_words$tf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:20,]
  cl <- cl[order(-cl$tf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p

```

HeatMap on tf-idf

```{r}
book_words <- book_words[order(-book_words$tf_idf),]
for (i in 1:max(book_words$clusters)){
  cl <- book_words[book_words$clusters==i,]
  cl <- cl[1:15,]
  cl <- cl[order(-cl$tf_idf),]
  if (i==1){cl_all <- cl}
  else {cl_all <-rbind(cl_all,cl)}
}

cl_all$word <- factor(cl_all$word, levels=rev(unique(cl_all$word)))
cl_all$clusters <- factor(cl_all$clusters, levels=unique(cl_all$clusters))

p <- ggplot(cl_all, aes(word, clusters)) +
  geom_tile(aes(fill = tf_idf),colour = "lightblue") +
  scale_fill_gradient(low = "lightblue",high = "darkblue")+
  coord_flip()
p



```

Let's save the results


```{r}
write.csv(cluster_1_words_m,"cluster_1_level_2_final_clusters.csv")
table(cluster_1_words_m$clusters)
```
